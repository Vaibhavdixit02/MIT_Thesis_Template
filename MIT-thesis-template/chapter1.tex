% From mitthesis package
% Version: 1.07, 2024/09/26
% Documentation: https://ctan.org/pkg/mitthesis

\chapter{Introduction}

\section{Mathematical Optimization: A Universal Language}

Optimization stands as one of the most fundamental paradigms in computational science, providing a universal framework for solving problems across disparate fields. At its core, optimization involves finding the best solution from a set of possible alternatives, typically by minimizing or maximizing an objective function subject to constraints. This conceptual simplicity belies the profound complexity that emerges when optimization is applied to real-world problems.

From the allocation of resources in economics to the training of deep neural networks in artificial intelligence, from the design of aircraft wings in engineering to the prediction of protein structures in computational biology, optimization serves as a common mathematical language. This universality stems from our natural tendency to seek optimal solutions—whether consciously through formal algorithms or unconsciously through evolutionary processes.

The mathematical formulation of an optimization problem typically takes the form:

\begin{equation}
\min_{x \in \mathcal{X}} f(x) \quad \text{subject to} \quad g_i(x) \leq 0, \, h_j(x) = 0
\end{equation}

where $f(x)$ represents the objective function to be minimized, $g_i(x)$ and $h_j(x)$ represent inequality and equality constraints respectively, and $\mathcal{X}$ defines the domain of the problem. This deceptively simple formulation encompasses an enormous variety of problems, from linear programming to complex non-convex optimization over manifolds.

Despite this universality, optimization problems vary tremendously in their computational tractability. Convex optimization problems—where both the objective function and feasible region are convex—possess favorable properties that guarantee global optimality of local solutions. These problems have been extensively studied, with efficient algorithms and robust software implementations widely available. However, many real-world problems are inherently non-convex, presenting significant challenges that form the primary focus of this thesis.

\section{The Challenge of Non-Convex Optimization}

Non-convex optimization problems permeate modern computational science and engineering. The training of deep neural networks, parameter estimation in complex dynamical systems, and many problems in statistical inference involve objective functions with multiple local minima, saddle points, and complex geometrical constraints.

The fundamental challenge of non-convex optimization lies in the gap between local and global optimality. In convex problems, any local minimum is also a global minimum, allowing optimization algorithms to converge to globally optimal solutions regardless of their starting point. In contrast, non-convex problems may contain numerous local minima, extended saddle points, and complex topological structures that trap optimization algorithms away from the global optimum.

These challenges manifest in several common scenarios:

\begin{enumerate}
\item \textbf{Multiple local minima}: Non-convex functions typically have many local minima, and determining which represents the global minimum can be computationally intractable.

\item \textbf{Saddle points}: High-dimensional optimization landscapes often contain saddle points where the gradient vanishes but the Hessian has both positive and negative eigenvalues. These points can significantly slow down gradient-based methods.

\item \textbf{Ill-conditioning}: Non-convex problems frequently exhibit regions where the condition number of the Hessian is extremely large, causing gradient-based methods to oscillate or progress extremely slowly.

\item \textbf{Plateaus and ravines}: Optimization trajectories often encounter nearly-flat regions or narrow valleys that are difficult to navigate efficiently.
\end{enumerate}

The practical consequences of these challenges are significant. In machine learning, they may lead to models that generalize poorly. In scientific computing, they may result in parameter estimates that fail to capture the true dynamics of the system. In engineering design, they may yield suboptimal solutions that waste resources or underperform.

Traditional approaches to non-convex optimization include global optimization methods like simulated annealing and genetic algorithms, which attempt to explore the entire solution space at the cost of computational efficiency, and local methods like gradient descent and quasi-Newton methods, which converge efficiently to local optima but may miss the global solution. Recently, stochastic methods have gained prominence, particularly in machine learning, for their ability to escape local minima and efficiently handle large datasets.

Despite these advances, non-convex optimization remains challenging, especially when problems involve additional structures like constraints or manifold domains. This thesis addresses these challenges through three complementary approaches: extending the notion of convexity to Riemannian manifolds, developing hybrid global-local optimization methods accelerated by GPU computing, and creating augmented Lagrangian methods with stochastic inner optimizers.

\section{Optimization on Manifolds}

Many optimization problems naturally arise on spaces that are not flat Euclidean spaces but curved manifolds. Examples include optimization over the set of rotation matrices in robotics and computer vision, positive definite matrices in statistics and engineering, and probability distributions in machine learning and information theory.

Manifold optimization extends traditional optimization techniques to these non-Euclidean spaces by respecting their intrinsic geometry. Rather than imposing constraints to keep iterates on the manifold, manifold optimization methods work directly on the manifold, using concepts like tangent spaces, retractions, and geodesics to generalize notions of descent directions and line searches.

The advantages of manifold optimization are both computational and conceptual. Computationally, working directly on the manifold often leads to more efficient algorithms by reducing the dimensionality of the problem and avoiding the ill-conditioning associated with enforcing constraints. Conceptually, manifold optimization provides a natural framework for problems where the solution space has inherent geometric structure.

However, manifold optimization introduces additional challenges. The curvature of the manifold affects the convexity properties of functions defined on it, and standard convexity in the Euclidean sense does not directly translate to manifolds. This has motivated the development of geodesic convexity—convexity along geodesics rather than straight lines—as a generalization of traditional convexity.

While geodesic convexity provides a theoretical foundation for optimization on manifolds, practical tools for verifying and exploiting this property have been limited. This thesis addresses this gap by introducing Disciplined Geodesically Convex Programming (DGCP), a framework that extends the successful concept of disciplined convex programming to the manifold setting.

\section{The Computational Perspective}

The theoretical study of optimization algorithms has traditionally focused on asymptotic properties like convergence rates and complexity bounds. While these analyses provide valuable insights, they often fail to capture the practical performance of algorithms on real-world problems. This thesis adopts a computational perspective that places equal emphasis on theoretical guarantees and practical implementation.

This perspective is informed by statistical learning theory, which studies the generalization capabilities of learning algorithms. In statistical learning, optimization serves as a means to an end—finding parameters that minimize empirical risk—but the ultimate goal is good performance on unseen data. This distinction highlights the importance of considering not just whether an algorithm converges to a local minimum, but how the properties of that minimum affect the overall performance of the model.

The computational perspective also acknowledges the importance of hardware considerations in modern optimization. The advent of GPU computing has dramatically accelerated certain types of numerical computations, particularly those involving dense linear algebra. However, effectively leveraging this hardware requires algorithms specifically designed for parallel execution. This thesis explores how GPU acceleration can be applied to global optimization methods, particularly for problems involving numerical simulations.

Furthermore, the computational perspective emphasizes the role of software in making theoretical advances accessible to practitioners. Sophisticated optimization algorithms are of limited use if they cannot be easily applied to real problems. This thesis therefore places significant emphasis on software implementation, providing tools that enable researchers and engineers to apply advanced optimization techniques without specialized knowledge of the underlying mathematics.

\section{Software for Scientific Computing and Machine Learning}

The landscape of scientific computing and machine learning has been transformed by the development of specialized software frameworks. These frameworks provide high-level abstractions that enable users to express complex computational problems concisely, while efficiently executing low-level operations on modern hardware.

In optimization, software frameworks serve several critical roles:

\begin{enumerate}
\item \textbf{Abstraction}: They provide a common interface for defining optimization problems, allowing users to focus on modeling rather than implementation details.

\item \textbf{Algorithm implementation}: They offer robust implementations of standard optimization algorithms, incorporating numerical safeguards and performance optimizations developed over decades of research.

\item \textbf{Differentiation}: Many frameworks provide automatic differentiation capabilities, computing exact gradients without requiring users to derive and implement them manually.

\item \textbf{Hardware acceleration}: They enable transparent use of specialized hardware like GPUs, often providing order-of-magnitude speedups for suitable problems.
\end{enumerate}

Traditional scientific computing frameworks like MATLAB and specialized optimization libraries like CPLEX and Gurobi have been staples in research and industry for decades. More recently, machine learning frameworks like TensorFlow and PyTorch have gained prominence, particularly for gradient-based optimization of neural networks.

These frameworks have significantly lowered the barrier to applying optimization techniques, but they often operate as "black boxes" that obscure the underlying algorithms. This can limit users' ability to understand and extend these methods for specialized applications. Additionally, the fragmentation of optimization software across different domains and languages creates artificial barriers between fields that could benefit from shared techniques.

\section{Julia and Optimization.jl}

The Julia programming language represents a significant advance in scientific computing, offering both high-level expressiveness and performance comparable to low-level languages like C and Fortran. Julia's design philosophy emphasizes composability, allowing independent libraries to work together seamlessly, and multiple dispatch, enabling generic programming without sacrificing performance.

These features make Julia particularly well-suited for optimization software. The ability to write high-performance code in a high-level language eliminates the traditional trade-off between development time and execution speed. The multiple dispatch system allows algorithms to be specialized for different problem structures without code duplication. And the composability of Julia packages enables optimization libraries to leverage advances in automatic differentiation, linear algebra, and other areas without reimplementing these capabilities.

Optimization.jl, the software framework developed as part of this thesis, embodies these principles to provide a unified interface to diverse optimization methods. Unlike previous frameworks that typically focus on specific classes of problems or algorithms, Optimization.jl offers a consistent interface to a wide range of optimization techniques, from traditional gradient-based methods to modern evolutionary algorithms, from unconstrained optimization to constrained problems on manifolds.

Key features of Optimization.jl include:

\begin{enumerate}
\item \textbf{Unified interface}: A common problem specification format that works across dozens of optimization algorithms, allowing users to experiment with different methods without changing their problem definition.

\item \textbf{Automatic differentiation integration}: Seamless support for multiple automatic differentiation backends, enabling efficient computation of gradients, Hessians, and other derivatives.

\item \textbf{Constraint handling}: A flexible constraint specification system that supports both equality and inequality constraints across multiple solvers.

\item \textbf{Extensibility}: A plugin system that allows new optimization algorithms to be integrated without modifications to the core package.

\item \textbf{Performance}: Carefully optimized implementations that leverage Julia's performance features to provide efficient execution, particularly for problems that involve numerical simulations.
\end{enumerate}

Optimization.jl has been adopted by researchers across numerous domains, including control systems, computational biology, econometrics, and machine learning. This broad adoption reflects both the universality of optimization problems and the practical utility of a unified software framework.

\section{Thesis Contributions and Organization}

This thesis addresses fundamental challenges in non-convex optimization through three complementary approaches:

\begin{enumerate}
\item \textbf{Disciplined Geodesically Convex Programming (DGCP)}: We introduce a framework for verifying and exploiting geodesic convexity on Riemannian manifolds, extending the concept of disciplined convex programming to non-Euclidean spaces. This framework enables automatic verification of problem structure and provides theoretical guarantees for a class of non-convex problems.

\item \textbf{GPU-Accelerated Hybrid Optimization}: We develop a hybrid optimization method that combines the global exploration capabilities of Particle Swarm Optimization with the efficient local convergence of L-BFGS. This method, accelerated by GPU computing, efficiently solves inverse problems with multiple local minima.

\item \textbf{Augmented Lagrangian with Stochastic Inner Optimizers}: We propose an augmented Lagrangian approach that incorporates stochastic optimizers like Adam for the inner minimization problems. This approach bridges traditional constrained optimization with modern machine learning techniques.
\end{enumerate}

Throughout these contributions, we emphasize software implementation, providing tools that make these advanced techniques accessible to practitioners. The software components are unified under the Optimization.jl framework, which provides a consistent interface to diverse optimization methods while enabling specialized implementations for particular problem classes.

In the following chapters, we delve into the theoretical foundations, algorithmic details, and practical applications of these approaches. Throughout, we maintain a computational perspective informed by statistical learning theory, emphasizing both theoretical guarantees and practical performance on real-world problems. Our goal is not only to advance the theoretical understanding of non-convex optimization but also to provide practical tools that researchers and engineers can readily apply to their specific domains.
